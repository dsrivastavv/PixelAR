# training related parameters
exp_name: "main"
results_dir: "outputs"
global_seed: 0

model:
  gpt_model: DGPT-B
  gpt_type: 'c2i'
  cls_token_num: 1
  dropout_p: 0.1
  drop_path_rate: 0.0
  token_dropout_p: 0.1
  vq_model: VQ-16
  codebook_size: 16384
  codebook_embed_dim: 8
  downsample_size: 16
  vq_ckpt: data/tokenizers/vq_ds16_c2i.pt
  embedding_type: dynamic_4d

dataset:
  code_path: data/res256/imagenet_code_c2i_flip_ten_crop
  image_size: 256
  num_classes: 1000

accelerator:
  gradient_accumulation_steps: 1
  mixed_precision: bf16
  inductor_mode: max-autotune-no-cudagraphs

patcher:
  patching_mode: entropy_row_boundary
  entropy_model_checkpoint_config: data/entropy_model/20250808_11/config.yaml
  entropy_model_checkpoint: data/entropy_model/20250808_11/checkpoints/epoch_00000299/model.safetensors
  log_time: False
  block_size: 16
  threshold: 7.8
  max_patch_length: 4

optimizer:
  lr: 1e-4
  weight_decay: 5e-2
  beta1: 0.9
  beta2: 0.95
  max_grad_norm: 1.0

training:
  num_workers: 16
  global_batch_size: 256
  epochs: 300
  wandb:
    wandb_entity: "LlamaGen"
    wandb_offline: True

checkpoint:
  log_every: 100
  log_with: wandb
  visualize_every: 10000 # every 10 log intervals, do one visualization
  visualize_num: 8
  disk_location: null

resume_dir: null
