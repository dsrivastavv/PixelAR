# training related parameters
exp_name: "entropy_model_384"
results_dir: "outputs"
global_seed: 0

model:
  gpt_model: GPT-B
  gpt_type: 'ucond'
  cls_token_num: 0
  dropout_p: 0.1
  drop_path_rate: 0.0
  token_dropout_p: 0.1
  vq_model: VQ-16
  codebook_size: 16384
  codebook_embed_dim: 8
  downsample_size: 16
  vq_ckpt: data/tokenizers/vq_ds16_c2i.pt

dataset:
  code_path: data/res384/imagenet_code_c2i_flip_ten_crop
  image_size: 384
  num_classes: 1000

accelerator:
  gradient_accumulation_steps: 1
  mixed_precision: bf16
  inductor_mode: max-autotune-no-cudagraphs

optimizer:
  lr: 1e-4
  weight_decay: 5e-2
  beta1: 0.9
  beta2: 0.95
  max_grad_norm: 1.0

training:
  num_workers: 24
  global_batch_size: 256
  epochs: 300
  wandb:
    wandb_entity: "EntropyModel"
    wandb_offline: True

checkpoint:
  log_every: 100
  log_with: wandb
  visualize_every: 10000 # every 10 log intervals, do one visualization
  visualize_num: 8
  disk_location: null

resume_dir: null